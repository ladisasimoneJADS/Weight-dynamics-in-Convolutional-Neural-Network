{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "keras.__version__\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import initializers\n",
    "\n",
    "#number of training session\n",
    "experiments = 30\n",
    "\n",
    "f1,f2,f3 = [],[],[]\n",
    "f1_container,f2_container,f3_container = [],[],[]\n",
    "\n",
    "for experiment in range(experiments):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), kernel_initializer= 'random_uniform', activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), kernel_initializer= 'random_uniform',  activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), kernel_initializer= 'random_uniform',  activation='relu'))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "    train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "    train_images = train_images.astype('float32') / 255\n",
    "\n",
    "    test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "    test_images = test_images.astype('float32') / 255\n",
    "\n",
    "    train_labels = to_categorical(train_labels)\n",
    "    test_labels = to_categorical(test_labels)\n",
    "    \n",
    "    #number of epochs for a single training\n",
    "    epochs = 10\n",
    "    \n",
    "    first_layer,second_layer, third_layer = [],[],[]\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "\n",
    "        model.fit(train_images, train_labels, epochs=1, batch_size=64)\n",
    "\n",
    "        check = 1\n",
    "        layer_number = 0\n",
    "\n",
    "        while check <= len(model.layers):\n",
    "            check += 1\n",
    "\n",
    "            for layer in model.layers:\n",
    "                W = layer.get_weights()\n",
    "\n",
    "                if 'Conv2D' in str(layer):\n",
    "                    layer_number +=1\n",
    "\n",
    "                    if (layer_number == 1):\n",
    "                        first_layer.append(W)\n",
    "                        continue\n",
    "\n",
    "                    if (layer_number == 2):\n",
    "                        second_layer.append(W)\n",
    "                        continue\n",
    "\n",
    "                    if (layer_number == 3):\n",
    "                        third_layer.append(W)\n",
    "                        continue\n",
    "    #store the weights at each epoch\n",
    "    '''with open(\"experiment_layers_update/experiment_{}_first_layer.csv\".format(experiment), \"w\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(first_layer)\n",
    "\n",
    "    with open(\"experiment_layers_update/experiment_{}_second_layer.csv\".format(experiment), \"w\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(second_layer)\n",
    "\n",
    "    with open(\"experiment_layers_update/experiment_{}_third_layer.csv\".format(experiment), \"w\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(third_layer)'''\n",
    "     \n",
    "     \n",
    "    # calculate the pairwise distances between the target (first_layer[-1][0]) and the weights at each epoch (first_layer[i][0])\n",
    "    first_container,second_container,third_container = [], [], []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        first_container.append(abs(first_layer[-1][0] - first_layer[i][0]).sum())\n",
    "        second_container.append(abs(second_layer[-1][0] - second_layer[i][0]).sum())\n",
    "        third_container.append(abs(third_layer[-1][0] - third_layer[i][0]).sum())\n",
    "    \n",
    "    #normilize the distance\n",
    "    maximum = first_container[0]\n",
    "    first = first_container/maximum\n",
    "    maximum_second = second_container[0]\n",
    "    second = second_container/maximum_second\n",
    "    maximum_third = third_container[0]\n",
    "    third = third_container/maximum_third\n",
    "    \n",
    "    #append the pairwise distances in f1_container, f2_container, f3_container\n",
    "    f1_container.append(first_container)\n",
    "    f2_container.append(second_container)\n",
    "    f3_container.append(third_container)\n",
    "    \n",
    "    #append the normalized distanced in f1, f2, f3\n",
    "    f1.append(first)\n",
    "    f2.append(second)\n",
    "    f3.append(third)\n",
    "    \n",
    "# store the normilized distances and the pairwise distances       \n",
    "'''\n",
    "with open(\"experiment_layers_update/experiment_first_layer_final.csv\", \"w\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(f1)\n",
    "\n",
    "with open(\"experiment_layers_update/experiment_second_layer_final.csv\", \"w\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(f2)\n",
    "\n",
    "with open(\"experiment_layers_update/experiment_third_layer_final.csv\", \"w\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(f3)    \n",
    "    \n",
    "with open(\"experiment_layers_update/experiment_first_layer_final_container.csv\", \"w\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(f1_container)\n",
    "\n",
    "with open(\"experiment_layers_update/experiment_second_layer_final_container.csv\", \"w\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(f2_container)\n",
    "\n",
    "with open(\"experiment_layers_update/experiment_third_layer_final_container.csv\", \"w\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(f3_container) '''\n",
    "    \n",
    "#converte the normilized distances and paiwise distances in tables    \n",
    "table_1 = pd.DataFrame(f1)\n",
    "table_2 = pd.DataFrame(f2)\n",
    "table_3 = pd.DataFrame(f3)\n",
    "table_1_container = pd.DataFrame(f1_container)\n",
    "table_2_container = pd.DataFrame(f2_container)\n",
    "table_3_container = pd.DataFrame(f3_container)\n",
    "\n",
    "#store and reload the tables\n",
    "'''\n",
    "table_1.to_csv(\"experiment_layers_update/table_layer_1.csv\", sep = '\\t', encoding = 'utf-8')\n",
    "table_2.to_csv(\"experiment_layers_update/table_layer_2.csv\", sep = '\\t', encoding = 'utf-8')\n",
    "table_3.to_csv(\"experiment_layers_update/table_layer_3.csv\", sep = '\\t', encoding = 'utf-8')\n",
    "table_1_container.to_csv(\"experiment_layers_update/table_layer_1_container.csv\", sep = '\\t', encoding = 'utf-8')\n",
    "table_2_container = pd.read_csv('experiment_layers_update/table_layer_2_container.csv', sep= '\\t', encoding = 'utf-8')\n",
    "table_3_container = pd.read_csv('experiment_layers_update/table_layer_3_container.csv', sep= '\\t', encoding = 'utf-8')\n",
    "\n",
    "table_1 = pd.read_csv('experiment_layers_update/table_layer_1.csv', sep= '\\t', encoding = 'utf-8')\n",
    "table_2 = pd.read_csv('experiment_layers_update/table_layer_2.csv', sep= '\\t', encoding = 'utf-8')\n",
    "table_3 = pd.read_csv('experiment_layers_update/table_layer_3.csv', sep= '\\t', encoding = 'utf-8')\n",
    "table_1_container = pd.read_csv('experiment_layers_update/table_layer_1_container.csv', sep= '\\t', encoding = 'utf-8')\n",
    "table_2_container = pd.read_csv('experiment_layers_update/table_layer_2_container.csv', sep= '\\t', encoding = 'utf-8')\n",
    "table_3_container = pd.read_csv('experiment_layers_update/table_layer_3_container.csv', sep= '\\t', encoding = 'utf-8')\n",
    "table_2_container.to_csv(\"experiment_layers_update/table_layer_2_container.csv\", sep = '\\t', encoding = 'utf-8')\n",
    "table_3_container.to_csv(\"experiment_layers_update/table_layer_3_container.csv\", sep = '\\t', encoding = 'utf-8')\n",
    "\n",
    "table_1 = table_1.drop(columns=['Unnamed: 0'])\n",
    "table_2 = table_2.drop(columns=['Unnamed: 0'])\n",
    "table_3 = table_3.drop(columns=['Unnamed: 0'])\n",
    "table_1_container = table_1_container.drop(columns=['Unnamed: 0'])\n",
    "table_2_container = table_2_container.drop(columns=['Unnamed: 0'])\n",
    "table_3_container = table_3_container.drop(columns=['Unnamed: 0']) '''\n",
    "\n",
    "# calculate mean and sstandard deciation over all the experiments \n",
    "a_1_mean=[]\n",
    "a_1_std = []\n",
    "for i in range(epochs):\n",
    "    a_1_mean.append(table_1[(i)].mean())\n",
    "    a_1_std.append(table_1[(i)].std())\n",
    "    \n",
    "a_2_mean=[]\n",
    "a_2_std = []\n",
    "for i in range(epochs):\n",
    "    a_2_mean.append(table_2[(i)].mean())\n",
    "    a_2_std.append(table_2[(i)].std())\n",
    "    \n",
    "a_3_mean=[]\n",
    "a_3_std = []\n",
    "for i in range(epochs):\n",
    "    a_3_mean.append(table_3[(i)].mean())\n",
    "    a_3_std.append(table_3[(i)].std())\n",
    "\n",
    "# plot mean and std for each layer at every epoch\n",
    "f = plt.figure(figsize=(15,9))\n",
    "f = plt.errorbar(range(1,epochs +1), a_1_mean, a_1_std, color='b', fmt='--o', ecolor='b', capthick=15, label = 'first layer')\n",
    "f = plt.errorbar(range(1,epochs +1), a_2_mean, a_2_std, color='g', fmt='--o', ecolor='g', capthick=2, label = 'second layer')\n",
    "f = plt.errorbar(range(1,epochs +1), a_3_mean, a_3_std, color='r', fmt='--o', ecolor='r', capthick=2, label = 'third layer')\n",
    "f = plt.xticks(range(1,epochs +1))\n",
    "f = plt.xlabel('Epoch')\n",
    "f = plt.ylabel('Δ Weights')\n",
    "f = plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "f = plt.title('Weight dynamics in Convolutional Neural Networks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
